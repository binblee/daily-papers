{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b015aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool \n",
    "# import packages that are used in our tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_hugging_face_top_daily_paper() -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the most upvoted paper on Hugging Face daily papers.\n",
    "    It returns the title of the paper\n",
    "    \"\"\"\n",
    "    try:\n",
    "      url = \"https://huggingface.co/papers\"\n",
    "      response = requests.get(url)\n",
    "      response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "      # Extract the title element from the JSON-like data in the \"data-props\" attribute\n",
    "      containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "      top_paper = \"\"\n",
    "\n",
    "      for container in containers:\n",
    "          data_props = container.get('data-props', '')\n",
    "          if data_props:\n",
    "              try:\n",
    "                  # Parse the JSON-like string\n",
    "                  json_data = json.loads(data_props.replace('&quot;', '\"'))\n",
    "                  if 'dailyPapers' in json_data:\n",
    "                      top_paper = json_data['dailyPapers'][0]['title']\n",
    "              except json.JSONDecodeError:\n",
    "                  continue\n",
    "\n",
    "      return top_paper\n",
    "    except requests.exceptions.RequestException as e:\n",
    "      print(f\"Error occurred while fetching the HTML: {e}\")\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f05ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the arxiv paper id by its title.\n",
    "    It returns the title of the paper\n",
    "\n",
    "    Args:\n",
    "        title: The paper title for which to get the id.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578be20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool gets the id of a paper and downloads it from arxiv. It saves the paper locally \n",
    "    in the current directory as \"paper.pdf\".\n",
    "\n",
    "    Args:\n",
    "        paper_id: The id of the paper to download.\n",
    "    \"\"\"\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    paper.download_pdf(filename=\"paper.pdf\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba340cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This function reads the first three pages of a PDF file and returns its content as a string.\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "    Returns:\n",
    "        A string containing the content of the PDF file.\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "    reader = PdfReader('paper.pdf')\n",
    "    print(len(reader.pages))\n",
    "    pages = reader.pages[:3]\n",
    "    for page in pages:\n",
    "        content += page.extract_text()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1a4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Summarize today's top paper on Hugging Face daily papers by reading it.</span>                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - dashscope/qwen-turbo ───────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSummarize today's top paper on Hugging Face daily papers by reading it.\u001b[0m                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - dashscope/qwen-turbo \u001b[0m\u001b[38;2;212;183;2m──────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_hugging_face_top_daily_paper()</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Top paper title: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                   </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_paper_id_by_title(title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title)</span><span style=\"background-color: #272822\">                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Paper ID: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                 </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_paper_by_id(paper_id</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id)</span><span style=\"background-color: #272822\">                                                                        </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper downloaded.\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                     </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> read_pdf_file(file_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"paper.pdf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"Paper content: {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                       </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_hugging_face_top_daily_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_paper_id_by_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper ID: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper_by_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                        \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper downloaded.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_pdf_file\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_path\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpaper.pdf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper content: \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title: MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n",
       "  Open Resources\n",
       "Paper ID: 2509.21268\n",
       "Paper downloaded.\n",
       "Paper content: Preprint. Work in progress.\n",
       "MMR1: ENHANCINGMULTIMODALREASONING WITH\n",
       "VARIANCE-AWARESAMPLING ANDOPENRESOURCES\n",
       "Sicong Leng1,2,∗ Jing Wang1,∗ Jiaxi Li3,∗ Hao Zhang2,∗ Zhiqiang Hu2\n",
       "Boqiang Zhang2 Yuming Jiang2 Hang Zhang2 Xin Li2 Lidong Bing2\n",
       "Deli Zhao2 Wei Lu1 Yu Rong2 Aixin Sun1,† Shijian Lu1,†\n",
       "1Nanyang Technological University\n",
       "2DAMO Academy, Alibaba Group\n",
       "3Singapore University of Technology and Design\n",
       "∗Equal Contributions †Correspondence\n",
       "ABSTRACT\n",
       "Large multimodal reasoning models have achieved rapid progress, but their ad-\n",
       "vancement is constrained by two major limitations: the absence of open, large-scale,\n",
       "high-quality long chain-of-thought (CoT) data, and the instability of reinforcement\n",
       "learning (RL) algorithms in post-training. Group Relative Policy Optimization\n",
       "(GRPO), the standard framework for RL fine-tuning, is prone togradient vanishing\n",
       "when reward variance is low, which weakens optimization signals and impairs con-\n",
       "vergence. This work makes three contributions: (1) We propose Variance-Aware\n",
       "Sampling (V AS), a data selection strategy guided by Variance Promotion Score\n",
       "(VPS) that combines outcome variance and trajectory diversity to promote reward\n",
       "variance and stabilize policy optimization. (2) We release large-scale, carefully\n",
       "curated resources containing ∼1.6M long CoT cold-start data and ∼15k RL QA\n",
       "pairs, designed to ensure quality, difficulty, and diversity, along with a fully repro-\n",
       "ducible end-to-end training codebase. (3) We open-source a family of multimodal\n",
       "reasoning models in multiple scales, establishing standardized baselines for the\n",
       "community. Experiments across mathematical reasoning benchmarks demonstrate\n",
       "the effectiveness of both the curated data and the proposed V AS. Comprehensive\n",
       "ablation studies and analyses provide further insight into the contributions of each\n",
       "component. In addition, we theoretically establish that reward variance lower-\n",
       "bounds the expected policy gradient magnitude, with V AS serving as a practical\n",
       "mechanism to realize this guarantee. Our code, data, and checkpoints are available\n",
       "athttps://github.com/LengSicong/MMR1.\n",
       "1 INTRODUCTION\n",
       "Recent advances in large language and multimodal reasoning models have markedly improved\n",
       "performance on complex tasks such as mathematics, science, and open-domain problem solving.\n",
       "Reinforcement learning (RL) plays a central role in these developments by optimizing models with\n",
       "process- or outcome-based rewards (Lightman et al., 2024; Wang et al., 2024c; Li et al., 2025b).\n",
       "Group Relative Policy Optimization (GRPO; Shao et al. (2024)) has emerged as a widely adopted RL\n",
       "framework due to its efficiency and scalability, and has been successfully applied to both language\n",
       "models (Guo et al., 2025) and multimodal models (Meng et al., 2025a; Wang et al., 2025b; Tan et al.,\n",
       "2025; Leng et al., 2025). However, GRPO is inherently susceptible togradient vanishing: when\n",
       "sampled rewards have low variance, relative advantages collapse toward zero, weakening optimization\n",
       "signals and destabilizing training (Razin et al., 2024; 2025). This issue persists across both unimodal\n",
       "and multimodal contexts, posing a fundamental challenge to effective RL optimization.\n",
       "In parallel, the progress of multimodal reasoning research is influenced by the limited availability of\n",
       "open, large-scale, high-quality long chain-of-thought (CoT) data. Compared with text-only reasoning,\n",
       "where multiple datasets are publicly accessible (Guha et al., 2025; Muennighoff et al., 2025), multi-\n",
       "modal training often relies on more restricted resources, which may constrain reproducibility and\n",
       "1\n",
       "arXiv:2509.21268v1  [cs.CV]  25 Sep 2025Preprint. Work in progress.\n",
       "training data\n",
       "random sampler\n",
       "weighted sampler\n",
       "trainingbatch mix\n",
       "policy model\n",
       "rolloutsrewards calculation\n",
       "update policysync policy &amp; re-calculate weights\n",
       "policy model\n",
       "rolloutsVPS calculation\n",
       "DataSampling GRPOTraining\n",
       "Figure 1:Overview of the Variance-Aware Sampling (V AS) framework.\n",
       "further development. Recent studies have made progress by exploring heuristic data curation (Meng\n",
       "et al., 2025a; Huang et al., 2025c; Chen et al., 2025a), reward design modifications (Tan et al., 2025;\n",
       "Shen et al., 2025), and training adjustments (Deng et al., 2025; Zhang et al., 2025b). While these\n",
       "approaches improve downstream performance, challenges related to stable GRPO optimization and\n",
       "the broader availability of curated multimodal reasoning data remain underexplored.\n",
       "In this work, we introduceVariance-AwareSampling (V AS), a dynamic data selection strategy\n",
       "designed to mitigate gradient vanishing in GRPO-based training for multimodal reasoning models.\n",
       "Our approach is grounded in the theoretical insight thatreward variance provides a lower bound on the\n",
       "expected policy gradient magnitude. Increasing reward variance, therefore, offers a principled means\n",
       "to stabilize training and strengthen policy optimization. Specifically, V AS employs theVariance\n",
       "PromotionScore (VPS), which evaluates each prompt’s potential to induce reward variance. VPS\n",
       "consists of two complementary components: theOutcomeVarianceScore (OVS), which favors\n",
       "prompts yielding a balanced mix of correct and incorrect responses to maximize expected reward\n",
       "variance, and theTrajectoryDiversityScore (TDS), which encourages diversity among reasoning\n",
       "trajectories, thereby raising the lower bound of variance and sustaining informative gradient signals\n",
       "even under sparse or noisy correctness feedback. Depicted in Figure 1, V AS constructs each training\n",
       "batch from two subsets: one sampled with probabilities proportional to VPS, emphasizing prompts\n",
       "with higher potential to induce reward variance, and another drawn uniformly at random to maintain\n",
       "broad data coverage. This design aims to balance targeted promotion of reward variance with general\n",
       "exposure to the training distribution. By introducing outcome- and trajectory-level variability into\n",
       "GRPO’s group-based comparisons, V AS reduces the risk of weak gradients and contributes to more\n",
       "stable optimization. A theoretical analysis is presented in §4.\n",
       "We validate V AS on a range of multimodal mathematical and logical reasoning benchmarks. Experi-\n",
       "ments show that V AS improves convergence, stability, and downstream performance. Ablation studies\n",
       "further demonstrate that OVS and TDS contribute complementary benefits: OVS enhances expected\n",
       "reward variance by balancing outcomes, while TDS increases trajectory diversity to support more\n",
       "consistent gradient updates. Beyond methodology, we curate and release large-scale datasets for both\n",
       "supervised fine-tuning and RL. The supervised dataset (∼1.6M) emphasizes long chain-of-thought\n",
       "reasoning paired with verified short answers, while the RL dataset (∼15k) is constructed to capture\n",
       "diverse levels of difficulty and domain coverage. Both datasets are curated with explicit attention to\n",
       "quality, difficulty, and diversity, ensuring their value for multimodal reasoning research. Together\n",
       "with these resources, we provide a reproducible codebase and open models at multiple scales, offering\n",
       "standardized baselines for future research.\n",
       "2 RELATEDWORK\n",
       "Building on the success of rule-based RL (Guo et al., 2025; Kimi, 2025), recent multimodal work\n",
       "explores RL with verifiable rewards, typically following a pipeline that conducts optional SFT\n",
       "activation then applies RL (Schulman et al., 2017; Ahmadian et al., 2024), such as GRPO (Shao\n",
       "et al., 2024), with fine-grained training recipes. In the multi-modal domain, various approaches\n",
       "2Preprint. Work in progress.\n",
       "refine this pipeline through specific reward design (Tan et al., 2025; Shen et al., 2025), sample\n",
       "diversification (Meng et al., 2025a; Wang et al., 2025b), hyperparameter tuning (Huang et al., 2025c;\n",
       "Tan et al., 2025; Yang et al., 2025a), and advanced RL strategies (Peng et al., 2025; Deng et al., 2025;\n",
       "Zhang et al., 2025b). Nevertheless, they often overlook the gradient vanishing problem inherent in\n",
       "GRPO-based training, resulting in unstable optimization and slow convergence (Razin et al., 2025).\n",
       "Some studies attempt to alleviate it by filtering samples with moderate pass rates (Wang et al., 2025b;\n",
       "Meng et al., 2025a), yet they remain largely heuristic and lack comprehensive experimental validation\n",
       "or theoretical grounding.\n",
       "Recent studies have examined gradient vanishing in a principled manner, analyzing how training\n",
       "objectives degrade under reward sparsity, variance reduction, and optimization dynamics (Liu et al.,\n",
       "2025; Hu et al., 2025; Vassoyan et al., 2025; Zhou et al., 2025). A range of remedies has been proposed,\n",
       "including reward rescaling (Li et al., 2024b; Huang et al., 2025a), entropy regularization (Liu et al.,\n",
       "2024), and improved sample selection (Wang et al., 2024d; Zhang et al., 2025c; Li et al., 2025a),\n",
       "which primarily operate by adjusting RL algorithms or reward mechanisms. In contrast, our work\n",
       "takes an orthogonal perspective by mitigating gradient vanishing through data sampling during\n",
       "training. Supported by both theoretical grounding and extensive empirical validation, our approach\n",
       "complements existing GRPO variants (Wang et al., 2025c; Liu et al., 2024; Hu et al., 2025; Yue et al.,\n",
       "2025) and can be naturally combined with them to further improve training stability and effectiveness\n",
       "in reasoning.\n",
       "3 VARIANCE-AWARESAMPLINGFRAMEWORK\n",
       "This section details the Variance-Aware Sampling (V AS) framework. §3.1 defines the Variance Promo-\n",
       "tion Score (VPS), comprising Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS),\n",
       "which guides dynamic data sampling. §3.2 then outlines the sampler implementation, including VPS\n",
       "updates and sample selection during training.\n",
       "3.1 VARIANCEPROMOTIONSCORE\n",
       "Let x be a prompt with ground-truth answer ¯yfrom the training set. In GRPO framework, the model\n",
       "generates N responses {yi}N\n",
       "i=1 for each x. A task-specific verifier V(x, yi,¯y)∈ {0,1}evaluates\n",
       "each response, returning1ify i matches¯yand0otherwise. Thepass rateforxis then defined as:\n",
       "P(x) = 1\n",
       "N\n",
       "NX\n",
       "i=1\n",
       "V(x, yi,¯y).\n",
       "Inspired by Foster &amp; Foerster (2025), we directly calculate the Outcome Variance Score (OVS) as:\n",
       "OVS(x) =P(x)(1−P(x)),\n",
       "which corresponds to the Bernoulli variance of correctness across responses. It is maximized at\n",
       "P(x) = 0.5 , where correct and incorrect outputs are balanced. We further define the Trajectory\n",
       "Diversity Score (TDS) to characterize variability in reasoning processes. Let Diversity({yi}N\n",
       "i=1) be a\n",
       "diversity function over sequences (e.g.,inverse self-BLEU or distinct-n):\n",
       "TDS(x) =Diversity({y i}N\n",
       "i=1),\n",
       "where a higher value reflects greater diversity among sampled trajectories. The overall Variance\n",
       "Promotion Score (VPS) is computed as a weighted combination of OVS and TDS:\n",
       "VPS(x) =α·OVS(x) +β·TDS(x),\n",
       "where α, β &gt;0balance their contributions. OVS increases the expected reward variance, while TDS\n",
       "provides a lower bound by encouraging trajectory diversity. Together, they are intended to strengthen\n",
       "the magnitude and consistency of gradient signals in GRPO training.\n",
       "3.2 DYNAMICSAMPLER\n",
       "The dynamic sampler prioritizes prompts with higher VPS, which are expected to induce greater\n",
       "reward variance during training. At each sampling step, the training batch is constructed from two\n",
       "3\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title: MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n",
       "  Open Resources\n",
       "Paper ID: 2509.21268\n",
       "Paper downloaded.\n",
       "Paper content: Preprint. Work in progress.\n",
       "MMR1: ENHANCINGMULTIMODALREASONING WITH\n",
       "VARIANCE-AWARESAMPLING ANDOPENRESOURCES\n",
       "Sicong Leng1,2,∗ Jing Wang1,∗ Jiaxi Li3,∗ Hao Zhang2,∗ Zhiqiang Hu2\n",
       "Boqiang Zhang2 Yuming Jiang2 Hang Zhang2 Xin Li2 Lidong Bing2\n",
       "Deli Zhao2 Wei Lu1 Yu Rong2 Aixin Sun1,† Shijian Lu1,†\n",
       "1Nanyang Technological University\n",
       "2DAMO Academy, Alibaba Group\n",
       "3Singapore University of Technology and Design\n",
       "∗Equal Contributions †Correspondence\n",
       "ABSTRACT\n",
       "Large multimodal reasoning models have achieved rapid progress, but their ad-\n",
       "vancement is constrained by two major limitations: the absence of open, large-scale,\n",
       "high-quality long chain-of-thought (CoT) data, and the instability of reinforcement\n",
       "learning (RL) algorithms in post-training. Group Relative Policy Optimization\n",
       "(GRPO), the standard framework for RL fine-tuning, is prone togradient vanishing\n",
       "when reward variance is low, which weakens optimization signals and impairs con-\n",
       "vergence. This work makes three contributions: (1) We propose Variance-Aware\n",
       "Sampling (V AS), a data selection strategy guided by Variance Promotion Score\n",
       "(VPS) that combines outcome variance and trajectory diversity to promote reward\n",
       "variance and stabilize policy optimization. (2) We release large-scale, carefully\n",
       "curated resources containing ∼1.6M long CoT cold-start data and ∼15k RL QA\n",
       "pairs, designed to ensure quality, difficulty, and diversity, along with a fully repro-\n",
       "ducible end-to-end training codebase. (3) We open-source a family of multimodal\n",
       "reasoning models in multiple scales, establishing standardized baselines for the\n",
       "community. Experiments across mathematical reasoning benchmarks demonstrate\n",
       "the effectiveness of both the curated data and the proposed V AS. Comprehensive\n",
       "ablation studies and analyses provide further insight into the contributions of each\n",
       "component. In addition, we theoretically establish that reward variance lower-\n",
       "bounds the expected policy gradient magnitude, with V AS serving as a practical\n",
       "mechanism to realize this guarantee. Our code, data, and checkpoints are available\n",
       "athttps://github.com/LengSicong/MMR1.\n",
       "1 INTRODUCTION\n",
       "Recent advances in large language and multimodal reasoning models have markedly improved\n",
       "performance on complex tasks such as mathematics, science, and open-domain problem solving.\n",
       "Reinforcement learning (RL) plays a central role in these developments by optimizing models with\n",
       "process- or outcome-based rewards (Lightman et al., 2024; Wang et al., 2024c; Li et al., 2025b).\n",
       "Group Relative Policy Optimization (GRPO; Shao et al. (2024)) has emerged as a widely adopted RL\n",
       "framework due to its efficiency and scalability, and has been successfully applied to both language\n",
       "models (Guo et al., 2025) and multimodal models (Meng et al., 2025a; Wang et al., 2025b; Tan et al.,\n",
       "2025; Leng et al., 2025). However, GRPO is inherently susceptible togradient vanishing: when\n",
       "sampled rewards have low variance, relative advantages collapse toward zero, weakening optimization\n",
       "signals and destabilizing training (Razin et al., 2024; 2025). This issue persists across both unimodal\n",
       "and multimodal contexts, posing a fundamental challenge to effective RL optimization.\n",
       "In parallel, the progress of multimodal reasoning research is influenced by the limited availability of\n",
       "open, large-scale, high-quality long chain-of-thought (CoT) data. Compared with text-only reasoning,\n",
       "where multiple datasets are publicly accessible (Guha et al., 2025; Muennighoff et al., 2025), multi-\n",
       "modal training often relies on more restricted resources, which may constrain reproducibility and\n",
       "1\n",
       "arXiv:2509.21268v1  [cs.CV]  25 Sep 2025Preprint. Work in progress.\n",
       "training data\n",
       "random sampler\n",
       "weighted sampler\n",
       "trainingbatch mix\n",
       "policy model\n",
       "rolloutsrewards calculation\n",
       "update policysync policy & re-calculate weights\n",
       "policy model\n",
       "rolloutsVPS calculation\n",
       "DataSampling GRPOTraining\n",
       "Figure 1:Overview of the Variance-Aware Sampling (V AS) framework.\n",
       "further development. Recent studies have made progress by exploring heuristic data curation (Meng\n",
       "et al., 2025a; Huang et al., 2025c; Chen et al., 2025a), reward design modifications (Tan et al., 2025;\n",
       "Shen et al., 2025), and training adjustments (Deng et al., 2025; Zhang et al., 2025b). While these\n",
       "approaches improve downstream performance, challenges related to stable GRPO optimization and\n",
       "the broader availability of curated multimodal reasoning data remain underexplored.\n",
       "In this work, we introduceVariance-AwareSampling (V AS), a dynamic data selection strategy\n",
       "designed to mitigate gradient vanishing in GRPO-based training for multimodal reasoning models.\n",
       "Our approach is grounded in the theoretical insight thatreward variance provides a lower bound on the\n",
       "expected policy gradient magnitude. Increasing reward variance, therefore, offers a principled means\n",
       "to stabilize training and strengthen policy optimization. Specifically, V AS employs theVariance\n",
       "PromotionScore (VPS), which evaluates each prompt’s potential to induce reward variance. VPS\n",
       "consists of two complementary components: theOutcomeVarianceScore (OVS), which favors\n",
       "prompts yielding a balanced mix of correct and incorrect responses to maximize expected reward\n",
       "variance, and theTrajectoryDiversityScore (TDS), which encourages diversity among reasoning\n",
       "trajectories, thereby raising the lower bound of variance and sustaining informative gradient signals\n",
       "even under sparse or noisy correctness feedback. Depicted in Figure 1, V AS constructs each training\n",
       "batch from two subsets: one sampled with probabilities proportional to VPS, emphasizing prompts\n",
       "with higher potential to induce reward variance, and another drawn uniformly at random to maintain\n",
       "broad data coverage. This design aims to balance targeted promotion of reward variance with general\n",
       "exposure to the training distribution. By introducing outcome- and trajectory-level variability into\n",
       "GRPO’s group-based comparisons, V AS reduces the risk of weak gradients and contributes to more\n",
       "stable optimization. A theoretical analysis is presented in §4.\n",
       "We validate V AS on a range of multimodal mathematical and logical reasoning benchmarks. Experi-\n",
       "ments show that V AS improves convergence, stability, and downstream performance. Ablation studies\n",
       "further demonstrate that OVS and TDS contribute complementary benefits: OVS enhances expected\n",
       "reward variance by balancing outcomes, while TDS increases trajectory diversity to support more\n",
       "consistent gradient updates. Beyond methodology, we curate and release large-scale datasets for both\n",
       "supervised fine-tuning and RL. The supervised dataset (∼1.6M) emphasizes long chain-of-thought\n",
       "reasoning paired with verified short answers, while the RL dataset (∼15k) is constructed to capture\n",
       "diverse levels of difficulty and domain coverage. Both datasets are curated with explicit attention to\n",
       "quality, difficulty, and diversity, ensuring their value for multimodal reasoning research. Together\n",
       "with these resources, we provide a reproducible codebase and open models at multiple scales, offering\n",
       "standardized baselines for future research.\n",
       "2 RELATEDWORK\n",
       "Building on the success of rule-based RL (Guo et al., 2025; Kimi, 2025), recent multimodal work\n",
       "explores RL with verifiable rewards, typically following a pipeline that conducts optional SFT\n",
       "activation then applies RL (Schulman et al., 2017; Ahmadian et al., 2024), such as GRPO (Shao\n",
       "et al., 2024), with fine-grained training recipes. In the multi-modal domain, various approaches\n",
       "2Preprint. Work in progress.\n",
       "refine this pipeline through specific reward design (Tan et al., 2025; Shen et al., 2025), sample\n",
       "diversification (Meng et al., 2025a; Wang et al., 2025b), hyperparameter tuning (Huang et al., 2025c;\n",
       "Tan et al., 2025; Yang et al., 2025a), and advanced RL strategies (Peng et al., 2025; Deng et al., 2025;\n",
       "Zhang et al., 2025b). Nevertheless, they often overlook the gradient vanishing problem inherent in\n",
       "GRPO-based training, resulting in unstable optimization and slow convergence (Razin et al., 2025).\n",
       "Some studies attempt to alleviate it by filtering samples with moderate pass rates (Wang et al., 2025b;\n",
       "Meng et al., 2025a), yet they remain largely heuristic and lack comprehensive experimental validation\n",
       "or theoretical grounding.\n",
       "Recent studies have examined gradient vanishing in a principled manner, analyzing how training\n",
       "objectives degrade under reward sparsity, variance reduction, and optimization dynamics (Liu et al.,\n",
       "2025; Hu et al., 2025; Vassoyan et al., 2025; Zhou et al., 2025). A range of remedies has been proposed,\n",
       "including reward rescaling (Li et al., 2024b; Huang et al., 2025a), entropy regularization (Liu et al.,\n",
       "2024), and improved sample selection (Wang et al., 2024d; Zhang et al., 2025c; Li et al., 2025a),\n",
       "which primarily operate by adjusting RL algorithms or reward mechanisms. In contrast, our work\n",
       "takes an orthogonal perspective by mitigating gradient vanishing through data sampling during\n",
       "training. Supported by both theoretical grounding and extensive empirical validation, our approach\n",
       "complements existing GRPO variants (Wang et al., 2025c; Liu et al., 2024; Hu et al., 2025; Yue et al.,\n",
       "2025) and can be naturally combined with them to further improve training stability and effectiveness\n",
       "in reasoning.\n",
       "3 VARIANCE-AWARESAMPLINGFRAMEWORK\n",
       "This section details the Variance-Aware Sampling (V AS) framework. §3.1 defines the Variance Promo-\n",
       "tion Score (VPS), comprising Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS),\n",
       "which guides dynamic data sampling. §3.2 then outlines the sampler implementation, including VPS\n",
       "updates and sample selection during training.\n",
       "3.1 VARIANCEPROMOTIONSCORE\n",
       "Let x be a prompt with ground-truth answer ¯yfrom the training set. In GRPO framework, the model\n",
       "generates N responses {yi}N\n",
       "i=1 for each x. A task-specific verifier V(x, yi,¯y)∈ {0,1}evaluates\n",
       "each response, returning1ify i matches¯yand0otherwise. Thepass rateforxis then defined as:\n",
       "P(x) = 1\n",
       "N\n",
       "NX\n",
       "i=1\n",
       "V(x, yi,¯y).\n",
       "Inspired by Foster & Foerster (2025), we directly calculate the Outcome Variance Score (OVS) as:\n",
       "OVS(x) =P(x)(1−P(x)),\n",
       "which corresponds to the Bernoulli variance of correctness across responses. It is maximized at\n",
       "P(x) = 0.5 , where correct and incorrect outputs are balanced. We further define the Trajectory\n",
       "Diversity Score (TDS) to characterize variability in reasoning processes. Let Diversity({yi}N\n",
       "i=1) be a\n",
       "diversity function over sequences (e.g.,inverse self-BLEU or distinct-n):\n",
       "TDS(x) =Diversity({y i}N\n",
       "i=1),\n",
       "where a higher value reflects greater diversity among sampled trajectories. The overall Variance\n",
       "Promotion Score (VPS) is computed as a weighted combination of OVS and TDS:\n",
       "VPS(x) =α·OVS(x) +β·TDS(x),\n",
       "where α, β >0balance their contributions. OVS increases the expected reward variance, while TDS\n",
       "provides a lower bound by encouraging trajectory diversity. Together, they are intended to strengthen\n",
       "the magnitude and consistency of gradient signals in GRPO training.\n",
       "3.2 DYNAMICSAMPLER\n",
       "The dynamic sampler prioritizes prompts with higher VPS, which are expected to induce greater\n",
       "reward variance during training. At each sampling step, the training batch is constructed from two\n",
       "3\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 14.15 seconds| Input tokens: 2,259 | Output tokens: 152]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 14.15 seconds| Input tokens: 2,259 | Output tokens: 152]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">summary </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The paper introduces Variance-Aware Sampling (VAS), a data selection strategy to mitigate gradient </span><span style=\"background-color: #272822\"> </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">vanishing in GRPO-based training for multimodal reasoning models. It proposes the Variance Promotion Score </span><span style=\"background-color: #272822\">    </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">(VPS), which combines Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS) to promote reward </span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">variance and stabilize policy optimization. The paper also releases large-scale datasets for supervised </span><span style=\"background-color: #272822\">       </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">fine-tuning and RL, along with a reproducible codebase and open models.\"</span><span style=\"background-color: #272822\">                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(summary)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe paper introduces Variance-Aware Sampling (VAS), a data selection strategy to mitigate gradient \u001b[0m\u001b[48;2;39;40;34m \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mvanishing in GRPO-based training for multimodal reasoning models. It proposes the Variance Promotion Score \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34m(VPS), which combines Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS) to promote reward \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mvariance and stabilize policy optimization. The paper also releases large-scale datasets for supervised \u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mfine-tuning and RL, along with a reproducible codebase and open models.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msummary\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The paper introduces Variance-Aware Sampling (VAS), a data selection strategy to mitigate gradient </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">vanishing in GRPO-based training for multimodal reasoning models. It proposes the Variance Promotion Score (VPS), </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">which combines Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS) to promote reward variance and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">stabilize policy optimization. The paper also releases large-scale datasets for supervised fine-tuning and RL, </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">along with a reproducible codebase and open models.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The paper introduces Variance-Aware Sampling (VAS), a data selection strategy to mitigate gradient \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mvanishing in GRPO-based training for multimodal reasoning models. It proposes the Variance Promotion Score (VPS), \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mwhich combines Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS) to promote reward variance and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mstabilize policy optimization. The paper also releases large-scale datasets for supervised fine-tuning and RL, \u001b[0m\n",
       "\u001b[1;38;2;212;183;2malong with a reproducible codebase and open models.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 4.19 seconds| Input tokens: 7,619 | Output tokens: 275]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 4.19 seconds| Input tokens: 7,619 | Output tokens: 275]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The paper introduces Variance-Aware Sampling (VAS), a data selection strategy to mitigate gradient vanishing in GRPO-based training for multimodal reasoning models. It proposes the Variance Promotion Score (VPS), which combines Outcome Variance Score (OVS) and Trajectory Diversity Score (TDS) to promote reward variance and stabilize policy optimization. The paper also releases large-scale datasets for supervised fine-tuning and RL, along with a reproducible codebase and open models.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, ToolCallingAgent\n",
    "from smolagents import InferenceClientModel, LiteLLMModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Override default api endpoint URL for DashScope in LiteLLMModel if your api key\n",
    "# is not -intl account.\n",
    "# DASHSCOPE_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1\n",
    "# DASHSCOPE_API_KEY=<your_api_key>\n",
    "\n",
    "model = LiteLLMModel(model_id=\"dashscope/qwen-turbo\")\n",
    "\n",
    "agent = CodeAgent(tools=[get_hugging_face_top_daily_paper,\n",
    "                         get_paper_id_by_title,\n",
    "                         download_paper_by_id,\n",
    "                         read_pdf_file],\n",
    "                  model=model,\n",
    "                  max_steps=10)\n",
    "\n",
    "agent.run(\n",
    "    \"Summarize today's top paper on Hugging Face daily papers by reading it.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily-papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
